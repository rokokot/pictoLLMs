library(psych)
library(caret)

PROP1

Score Kappa - annotator 1 vs 2 - compréhension picto
Number of subjects = 180 (9 participants x 20 pictos (1 obligatoire par phrase))

annotator1 : v (correct), x (incorrect, pas certain, ne sait pas) -> magali/auteur
annotator2 : v (correct), x (incorrect), na (pas certain ex. (v) ou (x), ne sait pas) -> expérimentateur/facilitateur

note : certaines réponses des deux annotateurs ont été jugées (in)correctes (v et x) après aide ou reformulation de l'expérimentateur/facilitateur

Cohen Kappa and Weighted Kappa correlation coefficients and confidence boundaries 
                 lower estimate upper
unweighted kappa  0.38     0.49  0.60
weighted kappa    0.26     0.40  0.53

-> kappa : 0.49 (modéré), 0.40 (faible)

Confusion Matrix and Statistics

          Reference
Prediction na  v  x
        na  0  0  0
        v  10 36 16
        x  11  9 98

Overall Statistics
                                          
               Accuracy : 0.7444          
                 95% CI : (0.6742, 0.8064)
    No Information Rate : 0.6333          
    P-Value [Acc > NIR] : 0.001006        
                                          
                  Kappa : 0.4876          
                                          
 Mcnemar's Test P-Value : 4.117e-05       

Statistics by Class:

                     Class: na Class: v Class: x
Sensitivity             0.0000   0.8000   0.8596
Specificity             1.0000   0.8074   0.6970
Pos Pred Value             NaN   0.5806   0.8305
Neg Pred Value          0.8833   0.9237   0.7419
Prevalence              0.1167   0.2500   0.6333
Detection Rate          0.0000   0.2000   0.5444
Detection Prevalence    0.0000   0.3444   0.6556
Balanced Accuracy       0.5000   0.8037   0.7783
Messages d'avis :
1: Dans levels(reference) != levels(data) :
  la taille d'un objet plus long n'est pas multiple de la taille d'un objet plus court
2: Dans confusionMatrix.default(data = annotator1_phrase, reference = annotator2_phrase) :
  Levels are not in the same order for reference and data. Refactoring data to match.

                annotator2_picto
annotator1_picto         na          v          x
               v 0.05555556 0.20000000 0.08888889
               x 0.06111111 0.05000000 0.54444444

annotator1_picto <- c ('x', 'v', 'x', 'v', 'x', 'v', 'x', 'v', 'x', 'v', 'v', 'v', 'x', 'x', 'x', 'x', 'v', 'x', 'x', 'x', 'x', 'v', 'x', 'v', 'v', 'v', 'x', 'v', 'x', 'v', 'x', 'v', 'x', 'x', 'v', 'v', 'v', 'v', 'x', 'v', 'x', 'x', 'x', 'v', 'v', 'v', 'x', 'v', 'x', 'v', 'x', 'v', 'x', 'x', 'x', 'x', 'v', 'v', 'x', 'v', 'x', 'x', 'x', 'v', 'v', 'v', 'x', 'v', 'x', 'v', 'x', 'v', 'x', 'x', 'x', 'v', 'v', 'v', 'x', 'v', 'x', 'x', 'x', 'v', 'x', 'x', 'x', 'v', 'v', 'x', 'x', 'v', 'x', 'x', 'x', 'x', 'v', 'v', 'x', 'x', 'x', 'v', 'x', 'v', 'x', 'x', 'x', 'v', 'x', 'v', 'x', 'x', 'v', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'v', 'x', 'v', 'x', 'v', 'x', 'v', 'x', 'v', 'x', 'x', 'x', 'v', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'v', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'v', 'x', 'v', 'x', 'v', 'x', 'v', 'x', 'v', 'x', 'x', 'x', 'x', 'x', 'x')
annotator2_picto <- c ('x', 'v', 'v', 'v', 'v', 'v', 'x', 'v', 'x', 'v', 'v', 'na', 'x', 'x', 'na', 'na', 'na', 'x', 'x', 'na', 'x', 'x', 'x', 'v', 'na', 'v', 'x', 'v', 'x', 'v', 'x', 'x', 'x', 'x', 'na', 'v', 'na', 'na', 'x', 'x', 'x', 'v', 'v', 'v', 'na', 'v', 'x', 'v', 'x', 'v', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'v', 'x', 'x', 'x', 'v', 'na', 'v', 'v', 'v', 'na', 'v', 'v', 'v', 'x', 'x', 'x', 'x', 'x', 'na', 'x', 'x', 'x', 'v', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'v', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'v', 'x', 'x', 'x', 'v', 'na', 'v', 'na', 'x', 'na', 'x', 'x', 'x', 'x', 'x', 'na', 'na', 'x', 'x', 'x', 'v', 'x', 'v', 'x', 'v', 'x', 'v', 'x', 'x', 'x', 'v', 'x', 'x', 'v', 'na', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'na', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'v', 'v', 'v', 'v', 'x', 'v', 'x', 'v', 'x', 'v', 'x', 'x', 'na', 'x', 'x', 'x')
cohen.kappa(x=cbind(annotator1_picto,annotator2_picto))

-----

PROP3

'na' -> NA (ignoré)

Cohen Kappa and Weighted Kappa correlation coefficients and confidence boundaries 
                 lower estimate upper
unweighted kappa   0.5     0.63  0.76
weighted kappa     0.5     0.63  0.76

 Number of subjects = 159
 
Confusion Matrix and Statistics

          Reference
Prediction  v  x
         v 36 16
         x  9 98
                                          
               Accuracy : 0.8428          
                 95% CI : (0.7767, 0.8956)
    No Information Rate : 0.717           
    P-Value [Acc > NIR] : 0.0001493       
                                          
                  Kappa : 0.63            
                                          
 Mcnemar's Test P-Value : 0.2301393       
                                          
            Sensitivity : 0.8000          
            Specificity : 0.8596          
         Pos Pred Value : 0.6923          
         Neg Pred Value : 0.9159          
             Prevalence : 0.2830          
         Detection Rate : 0.2264          
   Detection Prevalence : 0.3270          
      Balanced Accuracy : 0.8298          
                                          
       'Positive' Class : v

                annotator2_picto
annotator1_picto          v          x
               v 0.22641509 0.10062893
               x 0.05660377 0.61635220

annotator2_picto <- c ('x', 'v', 'v', 'v', 'v', 'v', 'x', 'v', 'x', 'v', 'v', NA, 'x', 'x', NA, NA, NA, 'x', 'x', NA, 'x', 'x', 'x', 'v', NA, 'v', 'x', 'v', 'x', 'v', 'x', 'x', 'x', 'x', NA, 'v', NA, NA, 'x', 'x', 'x', 'v', 'v', 'v', NA, 'v', 'x', 'v', 'x', 'v', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'v', 'x', 'x', 'x', 'v', NA, 'v', 'v', 'v', NA, 'v', 'v', 'v', 'x', 'x', 'x', 'x', 'x', NA, 'x', 'x', 'x', 'v', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'v', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'v', 'x', 'x', 'x', 'v', NA, 'v', NA, 'x', NA, 'x', 'x', 'x', 'x', 'x', NA, NA, 'x', 'x', 'x', 'v', 'x', 'v', 'x', 'v', 'x', 'v', 'x', 'x', 'x', 'v', 'x', 'x', 'v', NA, 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', NA, 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'v', 'v', 'v', 'v', 'x', 'v', 'x', 'v', 'x', 'v', 'x', 'x', NA, 'x', 'x', 'x')

annotator2_picto <- factor(annotator2_picto)
annotator12_picto <- rbind(annotator1_picto,annotator2_picto) #matrice de 180 (subjects) x 2 (raters)
> #Krippendorff's alpha
> # Subjects = 180
> # Raters = 2 
> # alpha = 0.63

kripp.alpha(annotator12_picto, "nominal") #ou kripp.alpha(annotator12_picto)
kripp.alpha(annotator12_picto, "ordinal") #équivalent

-----

PROP2

annotator2 : v (correct), x (incorrect), na (pas certain ex. (v) ou (x), ne sait pas) -> expérimentateur/facilitateur

Cohen Kappa and Weighted Kappa correlation coefficients and confidence boundaries 
                 lower estimate upper
unweighted kappa  0.41     0.54  0.67
weighted kappa    0.41     0.54  0.67

-> kappa : 0.54 (modéré)

Confusion Matrix and Statistics

          Reference
Prediction   v   x
         v  36  26
         x   9 109
                                          
               Accuracy : 0.8056          
                 95% CI : (0.7401, 0.8607)
    No Information Rate : 0.75            
    P-Value [Acc > NIR] : 0.048172        
                                          
                  Kappa : 0.5395          
                                          
 Mcnemar's Test P-Value : 0.006841        
                                          
            Sensitivity : 0.8000          
            Specificity : 0.8074          
         Pos Pred Value : 0.5806          
         Neg Pred Value : 0.9237          
             Prevalence : 0.2500          
         Detection Rate : 0.2000          
   Detection Prevalence : 0.3444          
      Balanced Accuracy : 0.8037          
                                          
       'Positive' Class : v  

                annotator2_picto_2
annotator1_picto         v         x
               v 0.2000000 0.1444444
               x 0.0500000 0.6055556

'na' -> 'x'
annotator2_picto_2 <- c ('x', 'v', 'v', 'v', 'v', 'v', 'x', 'v', 'x', 'v', 'v', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'v', 'x', 'v', 'x', 'v', 'x', 'v', 'x', 'x', 'x', 'x', 'x', 'v', 'x', 'x', 'x', 'x', 'x', 'v', 'v', 'v', 'x', 'v', 'x', 'v', 'x', 'v', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'v', 'x', 'x', 'x', 'v', 'x', 'v', 'v', 'v', 'x', 'v', 'v', 'v', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'v', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'v', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'v', 'x', 'x', 'x', 'v', 'x', 'v', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'v', 'x', 'v', 'x', 'v', 'x', 'v', 'x', 'x', 'x', 'v', 'x', 'x', 'v', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'v', 'v', 'v', 'v', 'x', 'v', 'x', 'v', 'x', 'v', 'x', 'x', 'x', 'x', 'x', 'x')

annotator1_picto <- factor(annotator1_picto)
annotator2_picto_2 <- factor(annotator2_picto_2)

cohen.kappa(x=cbind(annotator1_picto,annotator2_picto_2))
confusionMatrix(data=annotator1_picto, reference = annotator2_picto_2)

annotator12_picto <- rbind(annotator1_picto,annotator2_picto_2) #matrice de 180 (subjects) x 2 (raters)
> #Krippendorff's alpha
> # Subjects = 180
> # Raters = 2 
> # alpha = 0.536

kripp.alpha(annotator12_picto, "nominal") #ou kripp.alpha(annotator12_picto)
kripp.alpha(annotator12_picto, "ordinal") #équivalent
